
<!-- Optional: Center the image using HTML -->
<p align="start">

<a href="https://ibb.co/Fk4hWcQC"><img src="https://i.ibb.co/zWVm5C3D/Gemini-Generated-Image-vng7evng7evng7ev.png" alt="Gemini-Generated-Image-vng7evng7evng7ev" border="0"></a>
</p>


# ğŸ‘¦ğŸ» Auto Face Pose Capture using Logistic Regression


## ğŸš€ Problem Statement

In our computer vision application, we wanted to **add a personâ€™s face data into a database** by capturing it from a camera. Typically, this requires capturing images from multiple facial angles manually â€” such as:
- Left profile
- Right profile
- Upward (top) view
- Downward (bottom) view
- Center (frontal) view

Manually clicking images while instructing a person to rotate their head is time-consuming and error-prone.

### ğŸ¯ Objective

**Automate the face pose capturing process** using machine learning. Once the camera starts, the model should automatically identify the current head position and capture the face when it matches any of the five desired directions.

---

## ğŸ§  ML Solution

To solve this, we used **InsightFace's face pose estimation** features â€” extracting 3 key pose parameters:
- `yaw`
- `roll`
- `pitch`

Using these, we created a dataset where each face image was labeled based on its pose direction. Then, we trained a **Logistic Regression model** using a total of **150 features** derived from the face landmarks and pose data.

---

## ğŸ“Š Model Training & Evaluation

- âœ… Model: **Logistic Regression**
- ğŸ“ˆ Accuracy: **~90%**
- ğŸ”¢ Features Used: **150 pose-related features**
- ğŸ§ª Evaluation: Based on test set performance and live camera testing.

---

## ğŸ“¦ Project Structure
``` 
 â”œâ”€â”€app
    â”œâ”€â”€ captured_face_sides/            # all captured face images
    â”œâ”€â”€ configs/        # all configuration
    â”œâ”€â”€ data/           # all data related scripts
    â”œâ”€â”€ models/      
    â”œâ”€â”€ src/               
    â”œâ”€â”€ ..
    â”œâ”€â”€ .. 
    â”œâ”€â”€ live.py
    â”œâ”€â”€ main.py
           
â””â”€â”€ README.md            # Project documentation
```


---

## ğŸ¥ Demo Video

Watch the project demo here: [Demo Video on Google Drive](https://drive.google.com/file/d/1Y080ZjU1wUMvVBnU3cnAVLACD6Mqrv5x/view?usp=drive_link)


## ğŸ¥ Real-time Demo

Run the following to test the model on a live webcam feed:

```bash
git clone https://github.com/ak-123459/LookWhere.git
```

```bash
cd LookWhere
```

```bash
pip install -r requirements.txt
````


```bash
python app/live.py
```


### Train and validate the model

````
python app/main.py
````


## ğŸ§  Model
**Algorithm**: Logistic Regression

**Input**: Pose features (yaw, pitch, roll, etc.)

**Output**: Face direction label

## ğŸ“‚ Configuration
Hyperparameters and model configs can be found and edited in config.yaml.

## ğŸ“ƒ License
This project is licensed under the **MIT** License.




